{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bb5238-d3d0-47b9-8d28-b6e416dcc68e",
   "metadata": {},
   "source": [
    "# Тест пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc4f1b1e-260d-438a-9c70-968807ac62f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b4874-848f-4947-a61d-b1ae2efc5171",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43569028-fb9d-4e99-8c45-a660024ed525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "if torch.cuda.device_count() > 0:\n",
    "    gpu_available = True\n",
    "else:\n",
    "    gpu_available = False\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset1 = datasets.MNIST('/homes/flomakin/gun_crypto_system/data',\n",
    "                train=True, download=True,\n",
    "                transform=transform)\n",
    "\n",
    "dataset2 = datasets.MNIST('/homes/flomakin/gun_crypto_system/data',\n",
    "                train=False, download=True,\n",
    "                transform=transform)\n",
    "\n",
    "train_kwargs = {'batch_size': int(len(dataset1)/4)}\n",
    "test_kwargs = {'batch_size': len(dataset2)}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, drop_last=True, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, drop_last=True, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4db8d98-c552-455e-ae4b-a0b6b9b1de6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    i+=1\n",
    "    if i == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52822107-e355-4915-b724-d9f79fe69639",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8685573-bbe2-4f7b-bd6c-b5e673848d76",
   "metadata": {},
   "source": [
    "## Модель 1 - Энкодер Декодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e1bc57e-fc90-4746-a2a9-959c41c2fb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "conv2 = nn.Conv2d(32, 96, kernel_size=3, stride=2, padding=1)\n",
    "conv3 = nn.Conv2d(96, 288, kernel_size=3, stride=2, padding=1)\n",
    "conv4 = nn.Conv2d(288, 784, kernel_size=5, stride=2, padding=1)\n",
    "deconv4 = nn.ConvTranspose2d(784, 288, kernel_size=5, stride=2, padding=1, output_padding=1)\n",
    "deconv3 = nn.ConvTranspose2d(288, 96, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "deconv2 = nn.ConvTranspose2d(96, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "deconv1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c39a0fb5-935a-4c5e-9400-04d50c134ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image = torch.randn(60000, 1, 28, 28)\n",
    "image = data\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0833e0d-687a-4cf5-948c-c2c07c9bb20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = torch.randn(60000, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32708c0d-ccbf-42db-a3da-8df92611c3e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 1, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "70c3c753-650c-4cd6-b4e6-b7ac7f67695f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 32, 14, 14])\n",
      "torch.Size([60000, 96, 7, 7])\n",
      "torch.Size([60000, 288, 4, 4])\n",
      "torch.Size([60000, 784, 1, 1])\n",
      "torch.Size([60000, 288, 4, 4])\n",
      "torch.Size([60000, 96, 7, 7])\n",
      "torch.Size([60000, 32, 14, 14])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "a=conv1(image)\n",
    "print(a.shape)\n",
    "b=conv2(a)\n",
    "print(b.shape)\n",
    "c=conv3(b)\n",
    "print(c.shape)\n",
    "d = conv4(c)\n",
    "print(d.shape)\n",
    "e = deconv4(d)\n",
    "print(e.shape)\n",
    "f = deconv3(e)\n",
    "print(f.shape)\n",
    "g = deconv2(f)\n",
    "print(g.shape)\n",
    "h = deconv1(g)\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd65f5f-76c0-4d5f-b54b-188b7deee9db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d\u001b[38;5;241m.\u001b[39mview(d\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d.view(d.shape[0], 1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2bbd73-90d9-45ad-8f95-76e0d43b86b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d.view(d.shape[0], 1, -1).cuda().shape, k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96133fa-ac1b-4183-ae3a-174f948f0029",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcat([d\u001b[38;5;241m.\u001b[39mview(d\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), key], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cat([d.view(d.shape[0], 1, -1), key], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0ee1d7ee-0585-4925-a311-6467c69b396c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dod = d.view(d.shape[0], 1, 1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe78f509-c13f-49c3-b6ce-418eaf6cb74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Вход [1, 28, 28]\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 96, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(96, 288, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(288, 784, kernel_size=5, stride=2, padding=1)\n",
    "        # Выход [784, 1, 1]\n",
    "        \n",
    "        self.fc1 = nn.Linear(784 + 128, 784)\n",
    "        self.fc2 = nn.Linear(784, 784)\n",
    "        self.fc3 = nn.Linear(784, 784)\n",
    "        \n",
    "        # PReLU activation\n",
    "        self.prelu = nn.PReLU()\n",
    "        \n",
    "    def forward(self, x, k):\n",
    "        # Проход через сверточные слои\n",
    "        x = self.prelu(self.conv1(x))\n",
    "        x = self.prelu(self.conv2(x))\n",
    "        x = self.prelu(self.conv3(x))\n",
    "        x = self.prelu(self.conv4(x))\n",
    "        \n",
    "        # Примешиваем ключ\n",
    "        # x = torch.cat([x.view(x.shape[0], 1, 784), k], dim=1)\n",
    "        x = torch.cat([x.view(x.shape[0], 1, 784), k], dim=-1)\n",
    "        \n",
    "        x = self.prelu(self.fc1(x))\n",
    "        x = self.prelu(self.fc2(x))\n",
    "        x = nn.Tanh()(self.fc3(x)) \n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DecoderBOB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderBOB, self).__init__()\n",
    "\n",
    "        self.fc4 = nn.Linear(784 + 128, 784)\n",
    "        self.fc5 = nn.Linear(784, 784)\n",
    "        self.fc6 = nn.Linear(784, 784)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(784, 288, kernel_size=5, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(288, 96, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(96, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x, k):\n",
    "        x = self.prelu(self.fc4(torch.cat([x, k], dim=-1)))\n",
    "        x = self.prelu(self.fc5(x))\n",
    "        x = self.prelu(self.fc6(x))\n",
    "\n",
    "        x = x.view(x.shape[0], 784, 1, 1)\n",
    "\n",
    "        x = self.prelu(self.deconv4(x))\n",
    "        x = self.prelu(self.deconv3(x))\n",
    "        x = self.prelu(self.deconv2(x))\n",
    "        x = nn.Tanh()(self.deconv1(x)) \n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderEVA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderEVA, self).__init__()\n",
    "\n",
    "        self.fc4 = nn.Linear(784, 784)\n",
    "        self.fc5 = nn.Linear(784, 784)\n",
    "        self.fc6 = nn.Linear(784, 784)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(784, 288, kernel_size=5, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(288, 96, kernel_size=3, stride=2, padding=1, output_padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(96, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu(self.fc4(x))\n",
    "        x = self.prelu(self.fc5(x))\n",
    "        x = self.prelu(self.fc6(x))\n",
    "\n",
    "        x = x.view(x.shape[0], 784, 1, 1)\n",
    "\n",
    "        x = self.prelu(self.deconv4(x))\n",
    "        x = self.prelu(self.deconv3(x))\n",
    "        x = self.prelu(self.deconv2(x))\n",
    "        x = nn.Tanh()(self.deconv1(x))  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3b0de-d73b-4aeb-a435-4fa07307a26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ccb2c5f-546f-4c07-9672-3e8d9defd4d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en = Encoder()\n",
    "debob = DecoderBOB()\n",
    "deeva = DecoderEVA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea8659d-72f4-41db-a360-4db1436e1b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 1, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = data\n",
    "key = torch.randn(data.shape[0], 1, 128)\n",
    "encoded_output = en(image, key)\n",
    "encoded_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a78f637-cf10-4499-bb56-73561c79aa9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eva = deeva(encoded_output)\n",
    "eva.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05396a82-848c-4999-b261-95a5443dde94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15000, 1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob = debob(encoded_output, key)\n",
    "bob.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ecf1b-7243-47da-b404-936c2f1fc44c",
   "metadata": {},
   "source": [
    "## ЭКСП"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3fcef3-42e0-4a24-9799-c15ede970708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_steps = 1000\n",
    "learning_rate = 10e-3\n",
    "clip_value = 1\n",
    "def generate_key_batch(size: int = 128, batchsize: int = 1, gpu_available: bool = True):\n",
    "    if gpu_available:\n",
    "        return torch.randn(batchsize, 1, size).cuda()\n",
    "    else:\n",
    "        return torch.randn(batchsize, 1, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c239999-814a-4fab-810a-9922ff77f100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [03:17, 49.43s/it]\n"
     ]
    }
   ],
   "source": [
    "alice = Encoder()\n",
    "bob = DecoderBOB()\n",
    "eve = DecoderEVA()\n",
    "\n",
    "alice.train()\n",
    "bob.train()\n",
    "eve.train()\n",
    "\n",
    "if gpu_available:\n",
    "    alice.cuda()\n",
    "    bob.cuda()\n",
    "    eve.cuda()\n",
    "\n",
    "aggregated_losses = {\n",
    "        \"alice_bob_training_loss\": [],\n",
    "        \"bob_reconstruction_training_errors\": [],\n",
    "        \"eve_reconstruction_training_errors\": [],\n",
    "        \"step\": []\n",
    "}\n",
    "\n",
    "optimizer_alice = Adam(params=alice.parameters(), lr=learning_rate)\n",
    "optimizer_bob = Adam(params=bob.parameters(), lr=learning_rate)\n",
    "optimizer_eve = Adam(params=eve.parameters(), lr=learning_rate)\n",
    "\n",
    "# define losses \n",
    "bob_reconstruction_error = nn.L1Loss()\n",
    "eve_reconstruction_error = nn.L1Loss()\n",
    "\n",
    "for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "    data = data.to('cuda')\n",
    "    # Training alternates between Alice/Bob and Eve\n",
    "    for network, num_minibatches in {\"alice_bob\": 1, \"eve\": 2}.items():\n",
    "        \"\"\" \n",
    "        Alice/Bob training for one minibatch, and then Eve training for two minibatches this ratio \n",
    "        in order to give a slight computational edge to the adversary Eve without training it so much\n",
    "        that it becomes excessively specific to the exact current parameters of Alice and Bob\n",
    "        \"\"\"\n",
    "        for _ in range(num_minibatches):\n",
    "\n",
    "            k = generate_key_batch(size=128, batchsize=data.shape[0], gpu_available=gpu_available)\n",
    "\n",
    "            # forward pass through alice and eve networks\n",
    "\n",
    "            ciphertext = alice.forward(data, k)\n",
    "\n",
    "            eve_p = eve.forward(ciphertext)\n",
    "\n",
    "            if network == \"alice_bob\":\n",
    "\n",
    "                # forward pass through bob network\n",
    "                bob_p = bob.forward(ciphertext, k)\n",
    "\n",
    "                # calculate errors\n",
    "                error_bob = bob_reconstruction_error(input=bob_p, target=data)\n",
    "                error_eve = eve_reconstruction_error(input=eve_p, target=data)\n",
    "                alice_bob_loss =  error_bob + F.relu(1 - error_eve)\n",
    "\n",
    "                # Zero gradients, perform a backward pass, clip gradients, and update the weights.\n",
    "                optimizer_alice.zero_grad()\n",
    "                optimizer_bob.zero_grad()\n",
    "                alice_bob_loss.backward()\n",
    "                nn.utils.clip_grad_value_(alice.parameters(), clip_value)\n",
    "                nn.utils.clip_grad_value_(bob.parameters(), clip_value)\n",
    "                optimizer_alice.step()\n",
    "                optimizer_bob.step()\n",
    "\n",
    "            elif network == \"eve\":\n",
    "\n",
    "                # calculate error\n",
    "                error_eve = eve_reconstruction_error(input=eve_p, target=data)\n",
    "\n",
    "                # Zero gradients, perform a backward pass, and update the weights\n",
    "                optimizer_eve.zero_grad()\n",
    "                error_eve.backward()\n",
    "                nn.utils.clip_grad_value_(eve.parameters(), clip_value)\n",
    "                optimizer_eve.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df6256-f111-4615-b07b-7bd8e1b6a23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
